{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Data Science Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport albumentations as A\nimport torchvision.transforms as T\n\n\n!pip install openai-clip\n\n!pip install open_clip_torch\nimport open_clip\nimport cv2\nfrom PIL import Image\nimport yaml\nimport math\n\n# Model Creation Libraries\nimport os #to import custom models\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Model Training Libraries\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\n!pip install faiss-cpu\n!pip install faiss-gpu\nimport faiss\n#from tqdm import tqdm\nfrom tqdm import tqdm_notebook as tqdm\n#!pip install loguru\n#from loguru import logger\nfrom sklearn.preprocessing import normalize\nfrom torch.utils.data import ConcatDataset\n\nimport timm\nimport glob\nfrom transformers import (get_linear_schedule_with_warmup, \n                          get_constant_schedule,\n                          get_cosine_schedule_with_warmup, \n                          get_cosine_with_hard_restarts_schedule_with_warmup,\n                          get_constant_schedule_with_warmup)\nimport gc\nimport transformers\nfrom transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n\n#Error Handling Libraries\n!pip install pytorch-metric-learning\nfrom pytorch_metric_learning import losses\nfrom sklearn.model_selection import train_test_split\n\nimport copy\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport time\n#from torchvision import transforms\nimport random\n%load_ext autoreload\n%autoreload 2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-31T11:19:44.946911Z","iopub.execute_input":"2023-10-31T11:19:44.947227Z","iopub.status.idle":"2023-10-31T11:21:06.335862Z","shell.execute_reply.started":"2023-10-31T11:19:44.947198Z","shell.execute_reply":"2023-10-31T11:21:06.334797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:06.338232Z","iopub.execute_input":"2023-10-31T11:21:06.338615Z","iopub.status.idle":"2023-10-31T11:21:06.461922Z","shell.execute_reply.started":"2023-10-31T11:21:06.338581Z","shell.execute_reply":"2023-10-31T11:21:06.460825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#open_clip.list_pretrained()","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:06.463354Z","iopub.execute_input":"2023-10-31T11:21:06.464003Z","iopub.status.idle":"2023-10-31T11:21:06.558741Z","shell.execute_reply.started":"2023-10-31T11:21:06.463973Z","shell.execute_reply":"2023-10-31T11:21:06.557755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration Setup","metadata":{}},{"cell_type":"code","source":"class CFG:\n    model_name = 'ViT-H-14'           #Neural network model architecture\n    model_data = 'laion2b_s32b_b79k'  #Pretrained model\n    samples_per_class = 50            #Class balancing\n    n_classes = 0                     \n    min_samples = 4                   #Downsampling: criteria-diversity\n    image_size = 224                  #pixel 224 x 224\n    hidden_layer = 1024                #number of neurons in a hidden layer\n    seed = 5                         \n    workers = 12                      #number of CPU cores ; parallel tasks\n    train_batch_size = 4              \n    valid_batch_size = 8             \n    emb_size = 512                    \n    vit_bb_lr = {'10': 1.25e-6, '20': 2.5e-6, '26': 5e-6, '32': 10e-6}   #learning rates of backbone\n    vit_bb_wd = 1e-3                  #weight dacay of backbone\n    hd_lr = 3e-4                      \n    hd_wd = 1e-5                      \n    autocast = True                   \n    n_warmup_steps = 1000             \n    n_epochs = 10                     \n    device = torch.device('cuda')     \n    s=30.                             \n    m=0.45                            \n    m_min=0.05                        \n    acc_steps = 4                     \n    global_step = 0                   \n    reduce_lr = 0.1                   \n    crit = 'ce'                       #loss function cross entropy\n  ","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:06.559979Z","iopub.execute_input":"2023-10-31T11:21:06.56024Z","iopub.status.idle":"2023-10-31T11:21:06.657199Z","shell.execute_reply.started":"2023-10-31T11:21:06.560217Z","shell.execute_reply":"2023-10-31T11:21:06.656295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility Function","metadata":{}},{"cell_type":"markdown","source":"https://paperswithcode.com/method/arcface","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"class utilities():\n    class ArcMarginProduct(nn.Module):\n        \n        #Softmax Loss function extensiion - \"Additive Angular Margin Loss.\"\n        '''def __init__(self, dimension_of_input_features,  size_of_each_output_sample, scaling_factor_for_cosine_similarity, \n                     margin, easy_margin=False, ls_eps=0.0,computation-cuda)):\n        '''    \n        \"\"\"Implement of large margin arc distance: :\n            Args:\n                in_features: size of each input sample\n                out_features: size of each output sample\n                s: norm of input feature\n                m: margin\n                cos(theta + m)\n            \"\"\"\n        \"\"\"\n        exteded softmax loss fn. \n        simultaneously enhance the intra-class compactness and inter-class discrepancy\n        \"\"\"\n        def __init__(self, in_features, out_features, s=30.0, \n                     m=0.50, easy_margin=False, ls_eps=0.0, device=torch.device('cuda')):\n            super(ArcMarginProduct, self).__init__()\n            self.device = device\n            self.in_features = in_features\n            self.out_features = out_features\n            self.s = s\n            self.m = m\n            self.ls_eps = ls_eps  # label smoothing\n            self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n            nn.init.xavier_uniform_(self.weight)\n\n            self.easy_margin = easy_margin\n            self.cos_m = math.cos(m)\n            self.sin_m = math.sin(m)\n            self.th = math.cos(math.pi - m)\n            self.mm = math.sin(math.pi - m) * m\n        \n        #forward pass\n        def forward(self, input, label):\n            # --------------------------- cos(theta) & phi(theta) ---------------------\n            cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n            sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n            phi = cosine * self.cos_m - sine * self.sin_m\n            if self.easy_margin:\n                phi = torch.where(cosine > 0, phi, cosine)\n            else:\n                phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n            # --------------------------- convert label to one-hot ---------------------\n            # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n            # create a mask for the correct class\n            one_hot = torch.zeros(cosine.size(), device=self.device)\n            one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n            if self.ls_eps > 0:\n                one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n            # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n            output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n            output *= self.s\n\n            return output\n    \n    # Dense (per-class) cross-entropy loss for a classification task. \n    # loss fn for the classification between prediction and true. SGD based loss fn.\n    # DCE = -logprobs * target\n    class DenseCrossEntropy(nn.Module):\n        def forward(self, x, target):\n            x = x.float()\n            target = target.float()\n            logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n\n            loss = -logprobs * target\n            loss = loss.sum(-1)\n            return loss.mean()\n    \n    # loss fn in class imbalanced prob. \n    # FL(pt) = -target(1-probs)^(gamma)*(logprobs)\n    class FocalLoss(nn.Module):\n        def __init__(self, gamma=2):\n            super(FocalLoss, self).__init__()\n            self.gamma = gamma\n\n        def forward(self, x, target):\n            x = x.float()\n            target = target.float()\n            probs = torch.nn.functional.softmax(x, dim=-1)\n            logprobs = torch.log(probs)\n\n            loss = -logprobs * target * (1 - probs) ** self.gamma\n            loss = loss.sum(-1)\n            return loss.mean()\n    \n    #compute the cosine similarity scores between input features and a learnable set of \"center\" vectors.\n    class ArcMarginProduct_subcenter(nn.Module):\n        def __init__(self, in_features, out_features, k=3):\n            super().__init__()\n            self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n            self.reset_parameters()\n            self.k = k\n            self.out_features = out_features\n\n        def reset_parameters(self):\n            stdv = 1. / math.sqrt(self.weight.size(1))\n            self.weight.data.uniform_(-stdv, stdv)\n\n        def forward(self, features):\n            cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n            cosine_all = cosine_all.view(-1, self.out_features, self.k)\n            cosine, _ = torch.max(cosine_all, dim=2)\n            return cosine   \n    \n    class ArcFaceLossAdaptiveMargin(nn.modules.Module):\n        def __init__(self, margins, s=30.0, crit='ce'):\n            super().__init__()\n            if crit == 'ce':\n                self.crit = utilities.DenseCrossEntropy()\n            else:\n                self.crit = utilities.FocalLoss()\n            self.s = s\n            self.margins = margins\n            \n        def forward(self, logits, labels, out_dim):\n            ms = []\n            ms = self.margins[labels.cpu().numpy()]\n            cos_m = torch.from_numpy(np.cos(ms)).float().cuda()\n            sin_m = torch.from_numpy(np.sin(ms)).float().cuda()\n            th = torch.from_numpy(np.cos(math.pi - ms)).float().cuda()\n            mm = torch.from_numpy(np.sin(math.pi - ms) * ms).float().cuda()\n            labels = F.one_hot(labels, out_dim).float()\n            logits = logits.float()\n            cosine = logits\n            sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n            phi = cosine * cos_m.view(-1,1) - sine * sin_m.view(-1,1)\n            phi = torch.where(cosine > th.view(-1,1), phi, cosine - mm.view(-1,1))\n            output = (labels * phi) + ((1.0 - labels) * cosine)\n            output *= self.s\n            loss = self.crit(output, labels)\n            return loss     \n\n    def set_seed(seed):\n        '''Sets the seed of the entire notebook so results are the same every time we run.\n        This is for REPRODUCIBILITY.'''\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Set a fixed value for the hash seed\n        os.environ['PYTHONHASHSEED'] = str(seed)\n\n\n    def get_similiarity_hnsw(embeddings_gallery, emmbeddings_query, k):\n        \n        print('Processing indices...')\n\n        s = time.time()\n        index = faiss.IndexHNSWFlat(embeddings_gallery.shape[1], 32)\n        index.add(embeddings_gallery)\n\n        scores, indices = index.search(emmbeddings_query, k) \n        e = time.time()\n\n        print(f'Finished processing indices, took {e - s}s')\n        return scores, indices\n    \n    #Ecucledian Distance Similarity measure\n    def get_similiarity_l2(embeddings_gallery, emmbeddings_query, k):\n        print('Processing indices...')\n\n        s = time.time()\n        index = faiss.IndexFlatL2(embeddings_gallery.shape[1])\n        index.add(embeddings_gallery)\n\n        scores, indices = index.search(emmbeddings_query, k) \n        e = time.time()\n\n        print(f'Finished processing indices, took {e - s}s')\n        return scores, indices\n\n\n    def get_similiarity_IP(embeddings_gallery, emmbeddings_query, k):\n        print('Processing indices...')\n\n        s = time.time()\n        index = faiss.IndexFlatIP(embeddings_gallery.shape[1])\n        index.add(embeddings_gallery)\n\n        scores, indices = index.search(emmbeddings_query, k) \n        e = time.time()\n\n        print(f'Finished processing indices, took {e - s}s')\n        return scores, indices\n\n    def get_similiarity(embeddings, k):\n        print('Processing indices...')\n\n        index = faiss.IndexFlatL2(embeddings.shape[1])\n\n        res = faiss.StandardGpuResources()\n\n        index = faiss.index_cpu_to_gpu(res, 0, index)\n\n        index.add(embeddings)\n\n        scores, indices = index.search(embeddings, k) \n        print('Finished processing indices')\n\n        return scores, indices\n    @staticmethod\n    def map_per_image(label, predictions, k=5): \n        try:\n            return 1 / (predictions[:k].index(label) + 1)\n        except ValueError:\n            return 0.0\n    @staticmethod\n    def map_per_set(labels, predictions, k=5):\n        return np.mean([utilities.map_per_image(l, p, k) for l,p in zip(labels, predictions)])\n    \n    class AverageMeter(object):\n        \"\"\"Computes and stores the average and current value\"\"\"\n\n        def __init__(self, window_size=None):\n            self.length = 0\n            self.val = 0\n            self.avg = 0\n            self.sum = 0\n            self.count = 0\n            self.window_size = window_size\n\n        def reset(self):\n            self.length = 0\n            self.val = 0\n            self.avg = 0\n            self.sum = 0\n            self.count = 0\n\n        def update(self, val, n=1):\n            if self.window_size and (self.count >= self.window_size):\n                self.reset()\n            self.val = val\n            self.sum += val * n\n            self.count += n\n            self.avg = self.sum / self.count\n\n    def get_lr_groups(param_groups):\n            groups = sorted(set([param_g['lr'] for param_g in param_groups]))\n            groups = [\"{:2e}\".format(group) for group in groups]\n            return groups\n\n    def convert_indices_to_labels(indices, labels):\n        indices_copy = copy.deepcopy(indices)\n        for row in indices_copy:\n            for j in range(len(row)):\n                row[j] = labels[row[j]]\n        return indices_copy\n\n    class Multisample_Dropout(nn.Module):\n        def __init__(self, dropout_rate=0.1):\n            super(Multisample_Dropout, self).__init__()\n            self.dropout = nn.Dropout(dropout_rate)\n            self.dropouts = nn.ModuleList([nn.Dropout((i+1)*.1) for i in range(5)])\n\n        def forward(self, x, module):\n            x = self.dropout(x)\n            return torch.mean(torch.stack([module(dropout(x)) for dropout in self.dropouts],dim=0),dim=0) \n    \n    #Data augmentation\n    def transforms_auto_augment(image_path, image_size):\n        image = Image.open(image_path).convert('RGB')\n        train_transforms = transforms.Compose([transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET), transforms.PILToTensor()])\n        return train_transforms(image)\n\n    def transforms_cutout(image_path, image_size):\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n        train_transforms = A.Compose([\n                A.HorizontalFlip(p=0.5),\n                A.ImageCompression(quality_lower=99, quality_upper=100),\n                A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n                A.Resize(image_size, image_size),\n                A.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n                ToTensorV2(),\n            ])\n        return train_transforms(image=image)['image']\n\n    def transforms_happy_whale(image_path, image_size):\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n        aug8p3 = A.OneOf([\n                A.Sharpen(p=0.3),\n                A.ToGray(p=0.3),\n                A.CLAHE(p=0.3),\n            ], p=0.5)\n\n        train_transforms = A.Compose([\n                A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n                A.Resize(image_size, image_size),\n                aug8p3,\n                A.HorizontalFlip(p=0.5),\n                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                ToTensorV2(),\n            ])\n        return train_transforms(image=image)['image']\n\n    def transforms_valid(image_path, image_size):\n        image = Image.open(image_path).convert('RGB')\n        valid_transforms = transforms.Compose([transforms.PILToTensor()]) \n        return valid_transforms(image)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:06.65996Z","iopub.execute_input":"2023-10-31T11:21:06.66025Z","iopub.status.idle":"2023-10-31T11:21:06.806272Z","shell.execute_reply.started":"2023-10-31T11:21:06.660227Z","shell.execute_reply":"2023-10-31T11:21:06.805355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom IPython.display import Image\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:06.807797Z","iopub.execute_input":"2023-10-31T11:21:06.80809Z","iopub.status.idle":"2023-10-31T11:21:06.901192Z","shell.execute_reply.started":"2023-10-31T11:21:06.808065Z","shell.execute_reply":"2023-10-31T11:21:06.900287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Product-10K","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/visual-product-recognition/train.csv\")\ntrain_df['path'] = train_df.apply(lambda x: '/kaggle/input/visual-product-recognition/train/train' + '/' + x['name'], axis=1)\n\n# remove ../products-10k/test/9397815.jpg from the list!\ntest_df = pd.read_csv(\"/kaggle/input/visual-product-recognition/test.csv\")\ntest_df = test_df.drop(test_df[test_df.name == '9397815.jpg'].index) # smt wrong with this img\ntest_df['path'] = test_df.apply(lambda x: '/kaggle/input/visual-product-recognition/test/test' + '/' + x['name'], axis=1)\n\ndf = pd.concat([\n    test_df[['class','path']],\n    train_df[['class', 'path']]\n])\ndf_g = df.groupby('class', group_keys=True).apply(lambda x: x)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:06.902507Z","iopub.execute_input":"2023-10-31T11:21:06.903269Z","iopub.status.idle":"2023-10-31T11:21:10.555044Z","shell.execute_reply.started":"2023-10-31T11:21:06.903236Z","shell.execute_reply":"2023-10-31T11:21:10.553952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_g\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Open the image file\nimg = Image.open(df_g.iloc[0]['path'])\n\n# Display the image\nplt.imshow(img)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:10.556359Z","iopub.execute_input":"2023-10-31T11:21:10.556685Z","iopub.status.idle":"2023-10-31T11:21:11.041179Z","shell.execute_reply.started":"2023-10-31T11:21:10.556659Z","shell.execute_reply":"2023-10-31T11:21:11.040284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just to see;\nAI-CROWD Dataset","metadata":{}},{"cell_type":"code","source":"df_g_sampled = df_g.sample(n=10000, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:11.042357Z","iopub.execute_input":"2023-10-31T11:21:11.042648Z","iopub.status.idle":"2023-10-31T11:21:11.144282Z","shell.execute_reply.started":"2023-10-31T11:21:11.042622Z","shell.execute_reply":"2023-10-31T11:21:11.143344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_samples = []\nvalues_counts = []\nnum_classes = 0","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:11.145493Z","iopub.execute_input":"2023-10-31T11:21:11.145827Z","iopub.status.idle":"2023-10-31T11:21:11.23969Z","shell.execute_reply.started":"2023-10-31T11:21:11.145798Z","shell.execute_reply":"2023-10-31T11:21:11.238796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for group in tqdm(set(df_g_sampled['class'])):\n    names = list(df_g_sampled.path[df_g_sampled['class'] == group])\n    if len(names) >= CFG.min_samples:\n        paths = [\n            name for name in names[:CFG.samples_per_class]\n        ]\n\n        values_counts.append(len(paths))\n        training_samples.extend([\n            (p, num_classes) for p in paths\n        ])\n        \n        num_classes += 1","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:11.240912Z","iopub.execute_input":"2023-10-31T11:21:11.241242Z","iopub.status.idle":"2023-10-31T11:21:12.860256Z","shell.execute_reply.started":"2023-10-31T11:21:11.24121Z","shell.execute_reply":"2023-10-31T11:21:12.859299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = training_samples \nvalue_counts = np.array(values_counts)\nCFG.n_classes = num_classes\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:12.861651Z","iopub.execute_input":"2023-10-31T11:21:12.862264Z","iopub.status.idle":"2023-10-31T11:21:12.95661Z","shell.execute_reply.started":"2023-10-31T11:21:12.86223Z","shell.execute_reply":"2023-10-31T11:21:12.955649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\n# Open the image file\nimg = Image.open(data_train[500][0])\n\n# Display the image\nplt.imshow(img)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:12.957981Z","iopub.execute_input":"2023-10-31T11:21:12.95836Z","iopub.status.idle":"2023-10-31T11:21:13.567999Z","shell.execute_reply.started":"2023-10-31T11:21:12.958323Z","shell.execute_reply":"2023-10-31T11:21:13.567141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MOdel Development Train and Validation","metadata":{}},{"cell_type":"markdown","source":"### Clip Model","metadata":{}},{"cell_type":"markdown","source":"Define PyTorch models for the head of the Neura Network- for customizing the final layers of a deep learning model. \nModel expansion.\n\n- |__ **self.emb**: Linear (fully connected) layer - projects the input features to a lower-dimensional space (embedding space)\n- ||__ **self.dropout**: Custom dropout layer- dropout mask with multiple dropout rates.\n- |||__ **self.arc**: ArcMarginProduct layer - metric learning and class separation. It applies an adaptive margin-based loss to the embeddings.\n","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, hidden_size, k=3):\n        super(Head, self).__init__()\n        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n        self.dropout = utilities.Multisample_Dropout()\n        self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes, k)\n        \n    def forward(self, x):\n        embeddings = self.dropout(x, self.emb)\n        output = self.arc(embeddings)\n        return output, F.normalize(embeddings)\n    \nclass HeadV2(nn.Module):\n    def __init__(self, hidden_size, k=3):\n        super(HeadV2, self).__init__()\n        self.arc = utilities.ArcMarginProduct_subcenter(hidden_size, CFG.n_classes, k)\n        \n    def forward(self, x):\n        output = self.arc(x)\n        return output, F.normalize(x)\n    \nclass HeadV3(nn.Module):\n    def __init__(self, hidden_size, k=3):\n        super(HeadV3, self).__init__()        \n        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n        self.dropout = nn.Dropout1d(0.2)\n        self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes, k)\n        \n    def forward(self, x):\n        x = self.dropout(x)\n        x = self.emb(x)\n        output = self.arc(x)\n        return output, F.normalize(x)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:13.572587Z","iopub.execute_input":"2023-10-31T11:21:13.572961Z","iopub.status.idle":"2023-10-31T11:21:13.676927Z","shell.execute_reply.started":"2023-10-31T11:21:13.572919Z","shell.execute_reply":"2023-10-31T11:21:13.675982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MOdel Development ","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, vit_backbone, head_size, version='v1', k=3):\n        super(Model, self).__init__()\n        if version == 'v1':\n            self.head = Head(head_size, k)\n        elif version == 'v2':\n            self.head = HeadV2(head_size, k)\n        elif version == 'v3':\n            self.head = HeadV3(head_size, k)\n        else:\n            self.head = Head(head_size, k)\n        \n        self.encoder = vit_backbone.visual\n    def forward(self, x):\n        x = self.encoder(x)\n        return self.head(x)\n\n    def get_parameters(self):\n\n        parameter_settings = [] \n        parameter_settings.extend(\n            self.get_parameter_section(\n                [(n, p) for n, p in self.encoder.named_parameters()], \n                lr=CFG.vit_bb_lr, \n                wd=CFG.vit_bb_wd\n            )\n        ) \n\n        parameter_settings.extend(\n            self.get_parameter_section(\n                [(n, p) for n, p in self.head.named_parameters()], \n                lr=CFG.hd_lr, \n                wd=CFG.hd_wd\n            )\n        ) \n\n        return parameter_settings\n\n    def get_parameter_section(self, parameters, lr=None, wd=None): \n        parameter_settings = []\n\n\n        lr_is_dict = isinstance(lr, dict)\n        wd_is_dict = isinstance(wd, dict)\n\n        layer_no = None\n        for no, (n,p) in enumerate(parameters):\n            \n            for split in n.split('.'):\n                if split.isnumeric():\n                    layer_no = int(split)\n            \n            if not layer_no:\n                layer_no = 0\n            \n            if lr_is_dict:\n                for k,v in lr.items():\n                    if layer_no < int(k):\n                        temp_lr = v\n                        break\n            else:\n                temp_lr = lr\n\n            if wd_is_dict:\n                for k,v in wd.items():\n                    if layer_no < int(k):\n                        temp_wd = v\n                        break\n            else:\n                temp_wd = wd\n\n            weight_decay = 0.0 if 'bias' in n else temp_wd\n\n            parameter_setting = {\"params\" : p, \"lr\" : temp_lr, \"weight_decay\" : temp_wd}\n\n            parameter_settings.append(parameter_setting)\n\n            #print(f'no {no} | params {n} | lr {temp_lr} | weight_decay {weight_decay} | requires_grad {p.requires_grad}')\n\n        return parameter_settings","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:13.678181Z","iopub.execute_input":"2023-10-31T11:21:13.678524Z","iopub.status.idle":"2023-10-31T11:21:13.78284Z","shell.execute_reply.started":"2023-10-31T11:21:13.678496Z","shell.execute_reply":"2023-10-31T11:21:13.781809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ArcFace_criterion(logits_m, target, margins):\n    #print(\"Labels:\", target)\n    #print(\"Valid Margins:\", margins)\n    arc = utilities.ArcFaceLossAdaptiveMargin(margins=margins, s=CFG.s, crit=CFG.crit)\n    loss_m = arc(logits_m, target, CFG.n_classes)\n    return loss_m","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:13.784264Z","iopub.execute_input":"2023-10-31T11:21:13.784691Z","iopub.status.idle":"2023-10-31T11:21:13.87928Z","shell.execute_reply.started":"2023-10-31T11:21:13.784665Z","shell.execute_reply":"2023-10-31T11:21:13.878216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model & Validation Model\n","metadata":{}},{"cell_type":"markdown","source":"Given by the baseline code.\nOptimized it\n[train.py](https://gitlab.aicrowd.com/bartosz_ludwiczuk/visual-product-recognition-2023-starter-kit/-/blob/c0cb20339bc7443b83e62130d7ae3451a7d21a4e/MCS2023_baseline/train.py)","metadata":{}},{"cell_type":"code","source":"def train(model, train_loader, optimizer, scaler, scheduler, epoch):\n    \n    \"\"\"\n    Model training function for one epoch\n    :param model: model architecture\n    :param train_loader: dataloader for batch generation\n    :param criterion: selected criterion for calculating the loss function\n    :param optimizer: selected optimizer for updating weights\n    :param config: train process configuration\n    :param epoch (int): epoch number\n    :return: None\n    \"\"\"\n\n    \n    model.train()\n    loss_metrics = utilities.AverageMeter()\n    criterion = ArcFace_criterion\n\n    tmp = np.sqrt(1 / np.sqrt(value_counts))\n    margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * CFG.m + CFG.m_min\n        \n    bar = tqdm(train_loader)\n    for step, data in enumerate(bar):\n        step += 1\n        images = data['images'].to(CFG.device, dtype=torch.float)\n        labels = data['labels'].to(CFG.device)\n        batch_size = labels.size(0)\n        \n        with torch.cuda.amp.autocast(enabled=CFG.autocast):\n            outputs, features = model(images)\n\n        loss = criterion(outputs, labels//25, margins)\n        loss_metrics.update(loss.item(), batch_size)\n        loss = loss / CFG.acc_steps\n        scaler.scale(loss).backward()\n\n        if step % CFG.acc_steps == 0 or step == len(bar):\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n            CFG.global_step += 1\n                        \n        lrs = utilities.get_lr_groups(optimizer.param_groups)\n\n        loss_avg = loss_metrics.avg\n\n        bar.set_postfix(loss=loss_avg, epoch=epoch, lrs=lrs, step=CFG.global_step)\n\n        \n#wrapper, same inside repeated\n@torch.no_grad()\ndef validation(model, valid_loader):\n    \n    \"\"\"\n    Model validation function for one epoch\n    :param model: model architecture\n    :param val_loader: dataloader for batch generation\n    :param criterion: selected criterion for calculating the loss function\n    :param epoch (int): epoch number\n    :return: float: avg acc\n     \"\"\"\n\n    \n    model.eval() \n\n    all_embeddings = []\n    all_labels = [] \n\n    for data in tqdm(valid_loader):\n        images = data['images'].to(CFG.device, dtype=torch.float)\n        labels = data['labels'].to(CFG.device)\n\n        _, embeddings = model(images)\n\n        all_embeddings.append(embeddings.detach().cpu().numpy())\n        all_labels.append(labels.detach().cpu().numpy())\n\n\n    all_embeddings = np.concatenate(all_embeddings, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n\n    return all_embeddings, all_labels\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:13.880852Z","iopub.execute_input":"2023-10-31T11:21:13.881386Z","iopub.status.idle":"2023-10-31T11:21:13.984043Z","shell.execute_reply.started":"2023-10-31T11:21:13.881352Z","shell.execute_reply":"2023-10-31T11:21:13.983151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training(train_loader, \n             gallery_loader, \n             query_loader, \n             experiment_folder, \n             version='v1', \n             k=3, \n             reduce_lr_on_epoch=1,\n             use_rampup=True):\n    \n    \n    \n    backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, CFG.model_data)\n\n    model = Model(backbone, CFG.hidden_layer, version, k).to(CFG.device)\n    \n    optimizer = torch.optim.AdamW(model.get_parameters())\n \n    scaler = torch.cuda.amp.GradScaler(enabled=CFG.autocast)\n\n    steps_per_epoch = math.ceil(len(train_loader) / 100* CFG.acc_steps)\n\n    num_training_steps = math.ceil(CFG.n_epochs * steps_per_epoch)\n    \n    #print(backbone,model,optimizer,scaler,steps_per_epoch)\n    \n    if use_rampup:\n        scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                    num_training_steps=num_training_steps,\n                                                    num_warmup_steps=CFG.n_warmup_steps)  \n    else:\n        scheduler = get_constant_schedule(optimizer)\n        \n    best_score = 0\n    best_updated_ = 0\n    CFG.global_step = 0     \n    \n    for epoch in range(math.ceil(CFG.n_epochs)):\n        print(f'starting epoch {epoch}')\n\n        # train of product-10k\n        train(model, train_loader, optimizer, scaler, scheduler, epoch)\n        #print('x')\n        # aicrowd test data\n        print('gallery embeddings')\n        embeddings_gallery, labels_gallery = validation(model, gallery_loader)\n        print('query embeddings')\n        embeddings_query, labels_query = validation(model, query_loader)\n\n        \n        gc.collect()\n        torch.cuda.empty_cache() \n\n        # calculate validation score\n        _, indices = utilities.get_similiarity_l2(embeddings_gallery, embeddings_query, 1000)\n        \n        indices = indices.tolist()\n        \n        \n        labels_gallery = labels_gallery.tolist()\n        labels_query = labels_query.tolist()\n\n        preds = utilities.convert_indices_to_labels(indices, labels_gallery)\n        score = utilities.map_per_set(labels_query, preds)\n        print('validation score', score)\n\n        # early stopping\n        if score > best_score:\n            best_updated_ = 0\n            best_score = score\n\n        best_updated_ += 1\n\n        if best_updated_ >= 3:\n            print('no improvement done training....')\n            break\n            \n        if (epoch + 1) % reduce_lr_on_epoch == 0:\n            scheduler.base_lrs = [g['lr'] * CFG.reduce_lr for g in optimizer.param_groups]\n            \n        # to speed up the training\n        if epoch >= 1:\n            break\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:13.985546Z","iopub.execute_input":"2023-10-31T11:21:13.986019Z","iopub.status.idle":"2023-10-31T11:21:14.08844Z","shell.execute_reply.started":"2023-10-31T11:21:13.985987Z","shell.execute_reply":"2023-10-31T11:21:14.087502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loaders","metadata":{}},{"cell_type":"markdown","source":"Define two functions read_img::reading and preprocessing an image from a file and get_final_transform :: returns a sequence of image transformations that are commonly used in deep learning pipelines, particularly for image classification tasks","metadata":{}},{"cell_type":"code","source":"# data loader\n\ndef read_img(img_path, is_gray=False):\n    mode = cv2.IMREAD_COLOR if not is_gray else cv2.IMREAD_GRAYSCALE\n    img = cv2.imread(img_path, mode)\n    if not is_gray:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\ndef get_final_transform():  \n    final_transform = T.Compose([\n            T.Resize(\n                size=(CFG.image_size, CFG.image_size), \n                interpolation=T.InterpolationMode.BICUBIC,\n                antialias=True),\n            T.ToTensor(), \n            T.Normalize(\n                mean=(0.48145466, 0.4578275, 0.40821073), \n                std=(0.26862954, 0.26130258, 0.27577711)\n            )\n        ])\n    return final_transform\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:14.089604Z","iopub.execute_input":"2023-10-31T11:21:14.089874Z","iopub.status.idle":"2023-10-31T11:21:14.187209Z","shell.execute_reply.started":"2023-10-31T11:21:14.089851Z","shell.execute_reply":"2023-10-31T11:21:14.186294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defines a custom dataset class called ProductDataset. \nThis class is designed to be used with PyTorch for creating \na dataset of images and their corresponding labels, \nwhich can then be used with data loaders for training machine learning models\n\n","metadata":{}},{"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, \n                 data, \n                 transform=None, \n                 final_transform=None):\n        self.data = data\n        self.transform = transform\n        self.final_transform = final_transform\n            \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n       \n        img = read_img(self.data[idx][0])            \n        \n        if self.transform is not None:\n            if isinstance(self.transform, A.Compose):\n                img = self.transform(image=img)['image']\n            else:\n                img = self.transform(img)\n        \n        if self.final_transform is not None:\n            if isinstance(img, np.ndarray):\n                img =  Image.fromarray(img)\n            img = self.final_transform(img)\n            \n        product_id = self.data[idx][1]\n        return {\"images\": img, \"labels\": product_id}","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:14.18839Z","iopub.execute_input":"2023-10-31T11:21:14.188666Z","iopub.status.idle":"2023-10-31T11:21:14.286335Z","shell.execute_reply.started":"2023-10-31T11:21:14.188643Z","shell.execute_reply":"2023-10-31T11:21:14.28533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ImageNet-style data augmentation refers to the application of certain transformations to images in the ImageNet dataset to artificially expand the size of the training dataset. These transformations can include simple techniques such as cropping, rotating, and flipping input images.\n- AugMix is a data augmentation technique that improves the robustness and uncertainty estimates of image classifiers1. It is characterized by its utilization of simple augmentation operations in concert with a consistency loss1.\n- happy_whale data augmentation technique include rotations, scaling, color adjustments, and other transformations.\n- cut_out data augmentation,involves removing rectangular regions from images\n- clip data augmentation involving resizing and cropping.\n- clip+image_net Combines CLIP-style resizing and cropping with ImageNet-style data augmentation.\n\n[Data Augmentation](https://arxiv.org/abs/1712.04621v1)","metadata":{}},{"cell_type":"code","source":"\"\"\"\ngenerates a PyTorch DataLoader object \nfor training a deep learning model with an image dataset. \nThis function is used to create a data loader for a dataset, \nwith the option to apply different data augmentation strategies\n\"\"\"\ndef get_product_10k_dataloader(data_train, data_aug='image_net'):\n#def get_product_10k_dataloader(data_train, data_aug=None):\n    \n    transform = None\n    #ImageNet-style data augmentation\n    if data_aug == 'image_net':\n        transform = T.Compose([\n            T.ToPILImage(),\n            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET)\n        ])\n    #\n    elif data_aug == 'aug_mix':\n        transform = T.Compose([\n            T.ToPILImage(),\n            T.AugMix()\n        ])\n    #\n    elif data_aug == 'happy_whale':\n        aug8p3 = A.OneOf([\n            A.Sharpen(p=0.3),\n            A.ToGray(p=0.3),\n            A.CLAHE(p=0.3),\n        ], p=0.5)\n\n        transform = A.Compose([\n            A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n            A.Resize(CFG.image_size, CFG.image_size),\n            aug8p3,\n            A.HorizontalFlip(p=0.5),\n            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n        ])\n    #\n    elif data_aug == 'cut_out':        \n        transform = A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.ImageCompression(quality_lower=99, quality_upper=100),\n            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n            A.Resize(CFG.image_size, CFG.image_size),\n            A.Cutout(max_h_size=int(CFG.image_size * 0.4), \n                     max_w_size=int(CFG.image_size * 0.4), \n                     num_holes=1, p=0.5),\n        ])\n    elif data_aug == 'clip':\n        transform = T.Compose([\n            T.ToPILImage(),\n            T.RandomResizedCrop(\n                size=(224, 224), \n                scale=(0.9, 1.0), \n                ratio=(0.75, 1.3333), \n                interpolation=T.InterpolationMode.BICUBIC,\n                antialias=True\n            )\n        ])\n    elif data_aug == 'clip+image_net':\n        transform = T.Compose([\n            T.ToPILImage(),\n            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n            T.RandomResizedCrop(\n                size=(224, 224), \n                scale=(0.9, 1.0), \n                ratio=(0.75, 1.3333), \n                interpolation=T.InterpolationMode.BICUBIC,\n                antialias=True\n            )\n        ])\n    \n    final_transform = get_final_transform()\n    train_dataset = ProductDataset(data_train, \n                                   transform, \n                                   final_transform)\n    train_loader = DataLoader(train_dataset, \n                              batch_size = CFG.train_batch_size, \n                              num_workers=CFG.workers, \n                              shuffle=True, \n                              drop_last=True)\n    print(f'Training Data -> Dataset Length ({len(train_dataset)})')\n    return train_loader\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:14.287868Z","iopub.execute_input":"2023-10-31T11:21:14.28819Z","iopub.status.idle":"2023-10-31T11:21:14.394248Z","shell.execute_reply.started":"2023-10-31T11:21:14.288165Z","shell.execute_reply":"2023-10-31T11:21:14.393335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aicrowd_data_loader(csv_path, img_dir='/kaggle/input/products-10k/products-10k/development_test_data'):\n    df_g = pd.read_csv(csv_path)\n    df_g_ = df_g[['img_path', 'product_id']]\n    df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n    data_ = np.array(df_g_).tolist()\n    \n    final_transform = get_final_transform()\n    dataset = ProductDataset(data_, None, final_transform)\n    data_loader = DataLoader(dataset, \n                             batch_size = CFG.valid_batch_size, \n                             num_workers=CFG.workers, \n                             shuffle=False, \n                             drop_last=False)\n    \n    print(f'{csv_path} -> Dataset Length ({len(dataset)})')\n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:14.395508Z","iopub.execute_input":"2023-10-31T11:21:14.395889Z","iopub.status.idle":"2023-10-31T11:21:14.49409Z","shell.execute_reply.started":"2023-10-31T11:21:14.395857Z","shell.execute_reply":"2023-10-31T11:21:14.493145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AI-CROWD Dataset","metadata":{}},{"cell_type":"code","source":"gallery_loader = aicrowd_data_loader('/kaggle/input/products-10k/products-10k/development_test_data/gallery.csv') \nquery_loader = aicrowd_data_loader('/kaggle/input/products-10k/products-10k/development_test_data/queries.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:14.495511Z","iopub.execute_input":"2023-10-31T11:21:14.496142Z","iopub.status.idle":"2023-10-31T11:21:14.649946Z","shell.execute_reply.started":"2023-10-31T11:21:14.496106Z","shell.execute_reply":"2023-10-31T11:21:14.649057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set up and start the training process for a deep learning model, ","metadata":{}},{"cell_type":"markdown","source":"### --Training Start Here--","metadata":{}},{"cell_type":"code","source":"k = 3  \nversion = 'v2'\n#data_aug = None\ndata_aug = 'clip+image_net'\nCFG.reduce_lr = 0.1\ntrain_loader = get_product_10k_dataloader(data_train, data_aug)\nexperiment_folder = f'my_experiments/{CFG.model_name}-{CFG.model_data}-{str(data_aug)}-{str(version)}-p10k-Arcface(k={str(k)})-All-Epoch({str(CFG.n_epochs)})-Reduce_LR_0.1'\nprint(experiment_folder)\nmodel=training(train_loader, \n         gallery_loader, \n         query_loader, \n         experiment_folder, \n         version=version,\n         k=k)\n\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:21:14.651233Z","iopub.execute_input":"2023-10-31T11:21:14.651589Z","iopub.status.idle":"2023-10-31T11:37:42.288294Z","shell.execute_reply.started":"2023-10-31T11:21:14.651556Z","shell.execute_reply":"2023-10-31T11:37:42.287468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model weights\nmodel_save_path = \"weights.bin\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model weights saved to {model_save_path}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:49:07.727756Z","iopub.execute_input":"2023-10-31T11:49:07.728707Z","iopub.status.idle":"2023-10-31T11:49:10.908436Z","shell.execute_reply.started":"2023-10-31T11:49:07.72867Z","shell.execute_reply":"2023-10-31T11:49:10.907446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ---Final Phase---","metadata":{}},{"cell_type":"code","source":"#im_num=500 ##rememberdddd\n##Please don't change this 2-lines\nim_num= 300\n\nimport torch.optim as optim\nfrom IPython.display import Image\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ngallery_df = pd.read_csv(\"/kaggle/input/products-10k/products-10k/development_test_data/gallery.csv\")\ngallery_df['path'] = gallery_df.apply(lambda x: '/kaggle/input/products-10k/products-10k/development_test_data' + '/' + x['img_path'], axis=1)\n\nqueries_df = pd.read_csv(\"/kaggle/input/products-10k/products-10k/development_test_data/queries.csv\")\nqueries_df['path'] = queries_df.apply(lambda x: '/kaggle/input/products-10k/products-10k/development_test_data' + '/' + x['img_path'], axis=1)\n\n\nimg_q = Image.open(queries_df.loc[im_num]['path'])\nplt.imshow(img_q)\nplt.show()\n\ngallery_embeddings, _ = validation(model, gallery_loader)\nquery_embeddings, _ = validation(model, query_loader)\n\nembd_img = query_embeddings[im_num]\n#_, indices = utilities.get_similiarity_l2(gallery_embeddings,embd_img , k)\n\n# Reshape embd_img into a 2D array\nembd_img_2d = embd_img.reshape(1, -1)\n\n# Find the most similar images in the gallery for the query image\n_, indices = utilities.get_similiarity_l2(gallery_embeddings, embd_img_2d, k=15)\n\n# Flatten indices into a 1D list\nflat_indices = [index for sublist in indices for index in sublist]\n# Use flat_indices to select rows\nsimilar_images = gallery_df.loc[flat_indices]\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Create a new figure\nfig = plt.figure(figsize=(20, 20))\n\n# Loop over each row in the DataFrame\nfor i, row in enumerate(similar_images.iterrows()):\n    # Open the image file\n    img = Image.open(row[1]['path'])\n    \n    # Add a subplot for the current image\n    ax = fig.add_subplot(4, 5, i + 1)\n    \n    # Display the image in the subplot\n    ax.imshow(img)\n    \n    # Remove the axis\n    ax.axis('off')\n\n# Display the figure with all images\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:37:42.289559Z","iopub.execute_input":"2023-10-31T11:37:42.289844Z","iopub.status.idle":"2023-10-31T11:40:39.161713Z","shell.execute_reply.started":"2023-10-31T11:37:42.28982Z","shell.execute_reply":"2023-10-31T11:40:39.160504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom multiprocessing import Pool\n\n# Preload all images into a list\nall_images = [Image.open(path) for path in gallery_df['path']]\n\ndef load_image(i):\n    return all_images[i]\n\n# Use parallel processing to load images\nwith Pool() as p:\n    images = p.map(load_image, flat_indices)\n\n# Create a new figure\nfig = plt.figure(figsize=(20, 20))\n\n# Loop over each image\nfor i, img in enumerate(images):\n    # Add a subplot for the current image\n    ax = fig.add_subplot(4, 5, i + 1)\n    \n    # Display the image in the subplot\n    ax.imshow(img)\n    \n    # Remove the axis\n    ax.axis('off')\n\n# Display the figure with all images\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:40:39.163141Z","iopub.execute_input":"2023-10-31T11:40:39.163481Z","iopub.status.idle":"2023-10-31T11:40:42.929009Z","shell.execute_reply.started":"2023-10-31T11:40:39.163452Z","shell.execute_reply":"2023-10-31T11:40:42.927876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Code is not predicting labels for new data. Instead, its using the model to find similar images based on their embeddings","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom IPython.display import Image\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ngallery_df = pd.read_csv(\"/kaggle/input/products-10k/products-10k/development_test_data/gallery.csv\")\ngallery_df['path'] = gallery_df.apply(lambda x: '/kaggle/input/products-10k/products-10k/development_test_data' + '/' + x['img_path'], axis=1)\n\nqueries_df = pd.read_csv(\"/kaggle/input/products-10k/products-10k/development_test_data/queries.csv\")\nqueries_df['path'] = queries_df.apply(lambda x: '/kaggle/input/products-10k/products-10k/development_test_data' + '/' + x['img_path'], axis=1)\n\nim_num=150\nimg_q = Image.open(queries_df.loc[im_num]['path'])\nplt.imshow(img_q)\nplt.show()\n\nembd_img = query_embeddings[im_num]\n\n# Reshape embd_img into a 2D array\nembd_img_2d = embd_img.reshape(1, -1)\n\n# Find the most similar images in the gallery for the query image\n_, indices = utilities.get_similiarity_l2(gallery_embeddings, embd_img_2d, k=15)\n\n# Flatten indices into a 1D list\nflat_indices = [index for sublist in indices for index in sublist]\n# Use flat_indices to select rows\nsimilar_images = gallery_df.loc[flat_indices]\n\n# Initialize an empty dictionary\nimage_dict = {}\n\n# Loop over each row in the DataFrame\nfor i, row in enumerate(similar_images.iterrows()):\n    # Get the image path\n    img_path = row[1]['path']\n    \n    # Get the corresponding feature vector from gallery_embeddings\n    feature_vector = gallery_embeddings[flat_indices[i]]\n    \n    # Add the image path and feature vector to the dictionary\n    image_dict[img_path] = feature_vector\n\n# Now image_dict contains all the similar images' locations and their corresponding feature vectors\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Create a new figure\nfig = plt.figure(figsize=(20, 20))\n\n# Loop over each row in the DataFrame\nfor i, row in enumerate(similar_images.iterrows()):\n    # Open the image file\n    img = Image.open(row[1]['path'])\n    \n    # Add a subplot for the current image\n    ax = fig.add_subplot(4, 5, i + 1)\n    \n    # Display the image in the subplot\n    ax.imshow(img)\n    \n    # Remove the axis\n    ax.axis('off')\n\n# Display the figure with all images\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:46:45.214262Z","iopub.execute_input":"2023-10-31T11:46:45.214678Z","iopub.status.idle":"2023-10-31T11:46:48.517047Z","shell.execute_reply.started":"2023-10-31T11:46:45.214647Z","shell.execute_reply":"2023-10-31T11:46:48.51588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T11:42:32.429159Z","iopub.execute_input":"2023-10-31T11:42:32.430061Z","iopub.status.idle":"2023-10-31T11:42:32.535383Z","shell.execute_reply.started":"2023-10-31T11:42:32.430017Z","shell.execute_reply":"2023-10-31T11:42:32.534297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}